{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport re\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, random_split, DataLoader, \\\n                             RandomSampler, SequentialSampler\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\ntorch.manual_seed(42)\n\nfrom transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n                         AdamW, get_linear_schedule_with_warmup, \\\n                         TrainingArguments, BeamScorer, Trainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n                         AdamW, get_linear_schedule_with_warmup, \\\n                         TrainingArguments, BeamScorer, Trainer\n\nimport torch\nfrom torch.utils.data import Dataset, random_split, DataLoader, \\\n                             RandomSampler, SequentialSampler\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n                         AdamW, get_linear_schedule_with_warmup, \\\n                         TrainingArguments, BeamScorer, Trainer\n\nimport torch\nfrom torch.utils.data import Dataset, random_split, DataLoader, \\\n                             RandomSampler, SequentialSampler\n## **GPT-2 with feature: title, structure**\nidea comes from : https://colab.research.google.com/drive/13dZVYEOMhXhkXWfvSMVM1TTtUDrT6Aeh?usp=sharing#scrollTo=vCPohrZ-CTWu","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/630-final/wn21630project_input_processed.csv')\ndf.fillna(' ',inplace = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\nwith open('./train_dataset.txt','w') as f:\n    f.write(''.join(d[:int(length*0.6)]))\nwith open('./dev_dataset.txt','w') as f:\n    f.write(' <Newsong> '.join(d[int(length*0.6):int(length*0.8)]))\nwith open('./test_dataset.txt','w') as f:\n    f.write(' <Newsong> '.join(d[int(length*0.8):]))\ntrain_path = './train_dataset.txt'\ndev_path = './dev_dataset.txt'","metadata":{}},{"cell_type":"code","source":"SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n                    \"eos_token\": \"<|EOS|>\",\n                    \"unk_token\": \"<|UNK|>\",                    \n                    \"pad_token\": \"<|PAD|>\",\n                    \"sep_token\": \"<|SEP|>\"}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"keywords = ['intro', 'verse', 'chorus', 'bridge', 'hook']\ninput = []\nfor index,row in df.iterrows():\n    key_i, text_i = [],[]\n    for i in keywords:\n        if row[i] !=' ':\n            key_i.append(i)\n            text_i.append(re.sub('(\\n)+','. ',row[i]))\n    input_i = SPECIAL_TOKENS['bos_token'] + row.title + \\\n        SPECIAL_TOKENS['sep_token'] + ', '.join(key_i) + SPECIAL_TOKENS['sep_token'] + \\\n        '. '.join(text_i) + SPECIAL_TOKENS['eos_token']\n    input.append(input_i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"length = len(input)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('./train_dataset.txt','w') as f:\n    f.write(' <Newsong> '.join(input[:int(length*0.6)]))\nwith open('./dev_dataset.txt','w') as f:\n    f.write(' <Newsong> '.join(input[int(length*0.6):int(length*0.8)]))\nwith open('./test_dataset.txt','w') as f:\n    f.write(' <Newsong> '.join(input[int(length*0.8):]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = './train_dataset.txt'\ndev_path = './dev_dataset.txt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class myDataset(Dataset):\n\n    def __init__(self, data, tokenizer):\n        keywords = ['intro', 'verse', 'chorus', 'bridge', 'hook']\n        input = []\n        for index,row in data.iterrows():\n            key_i, text_i = [],[]\n            for i in keywords:\n                if row[i] !=' ':\n                    key_i.append(i)\n                    text_i.append(re.sub('(\\n)+',' <Newline> ',row[i]))\n            input_i = SPECIAL_TOKENS['bos_token'] + row.title + \\\n                SPECIAL_TOKENS['sep_token'] + ', '.join(key_i) + SPECIAL_TOKENS['sep_token'] + \\\n                ' <Newsong> '.join(text_i) + SPECIAL_TOKENS['eos_token']\n            input.append(input_i)\n \n        \n        self.tokenizer = tokenizer \n        self.text      = input\n\n    #---------------------------------------------#\n\n    def __len__(self):\n        return len(self.text)\n\n    #---------------------------------------------#\n    \n    def __getitem__(self,i):\n        encodings_dict = tokenizer(self.text[i],                                   \n                                   truncation=True, \n                                   max_length=MAXLEN, \n                                   padding=\"max_length\")   \n        \n        input_ids = encodings_dict['input_ids']\n        attention_mask = encodings_dict['attention_mask']\n        \n        return {'label': torch.tensor(input_ids),\n                'input_ids': torch.tensor(input_ids), \n                'attention_mask': torch.tensor(attention_mask)}","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_size = 1070\ndeving_size = 357\ntesting_size = 358\nsize = [training_size, deving_size,testing_size]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEBUG           = False\n\nINPUT_DIR       = 'articles'\n\nUSE_APEX        = True\nAPEX_OPT_LEVEL  = 'O1'\n\nMODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n\nUNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n\n\n                    \nMAXLEN          = 768  #{768, 1024, 1280, 1600}\n\nTRAIN_SIZE      = 0.8\n\nif USE_APEX:\n    TRAIN_BATCHSIZE = 4\n    BATCH_UPDATE    = 16\nelse:\n    TRAIN_BATCHSIZE = 2\n    BATCH_UPDATE    = 32\n\nEPOCHS          = 4\nLR              = 5e-4\nEPS             = 1e-8\nWARMUP_STEPS    = 1e2\n\nSEED            = 2020","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_tokenier(special_tokens=None):\n    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n\n    if special_tokens:\n        tokenizer.add_special_tokens(special_tokens)\n        print(\"Special tokens added\")\n    return tokenizer\n\ndef get_model(tokenizer, special_tokens=None, load_model_path=None):\n\n    #GPT2LMHeadModel\n    if special_tokens:\n        config = AutoConfig.from_pretrained(MODEL, \n                                            bos_token_id=tokenizer.bos_token_id,\n                                            eos_token_id=tokenizer.eos_token_id,\n                                            sep_token_id=tokenizer.sep_token_id,\n                                            pad_token_id=tokenizer.pad_token_id,\n                                            output_hidden_states=False)\n    else: \n        config = AutoConfig.from_pretrained(MODEL,                                     \n                                            pad_token_id=tokenizer.eos_token_id,\n                                            output_hidden_states=False)    \n\n    #----------------------------------------------------------------#\n    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n\n    if special_tokens:\n        #Special tokens added, model needs to be resized accordingly\n        model.resize_token_embeddings(len(tokenizer))\n\n    if load_model_path:\n        model.load_state_dict(torch.load(load_model_path))\n\n    model.cuda()\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\nmodel = get_model(tokenizer, \n                  special_tokens=SPECIAL_TOKENS,\n                #   load_model_path='pytorch_model.bin'\n                 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = df.iloc[:training_size]\nval_data = df.iloc[training_size:(deving_size+training_size)]\n\ntrain_dataset = myDataset(train_data, tokenizer)\nval_dataset = myDataset(val_data, tokenizer)\n","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TextDataset,DataCollatorForLanguageModeling\n\ndef load_dataset(dev_path,test_path,tokenizer):\n    train_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=train_path,\n          block_size=32)\n        #   block_size=128)\n     \n    dev_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=dev_path,\n          block_size=32) \n        #   block_size=128)  \n    \n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=False,\n    )\n    return train_dataset,dev_dataset,data_collator\n\ntrain_dataset,dev_dataset,data_collator = load_dataset(train_path,dev_path,tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nfrom transformers import GPT2LMHeadModel\n#model = GPT2LMHeadModel.from_pretrained('gpt2')\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./output\", #The output directory\n    overwrite_output_dir=True, #overwrite the content of the output directory\n    num_train_epochs=4, # number of training epochs\n    per_device_train_batch_size=32, # batch size for training\n    per_device_eval_batch_size=64,  # batch size for evaluation\n    # per_device_train_batch_size=16, # batch size for training\n    # per_device_eval_batch_size=32,  # batch size for evaluation\n    eval_steps = 400, # Number of update steps between two evaluations.\n    save_steps=800, # after # steps model is saved \n    warmup_steps=500,# number of warmup steps for learning rate scheduler\n    prediction_loss_only=True,\n    )\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=dev_dataset,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"from transformers import TextDataset,DataCollatorForLanguageModeling\ndata = df\ntext, keylist, title = [],[],[]\nkeywords = ['intro', 'verse', 'chorus', 'bridge', 'hook']\n\nfor index, row in data.iterrows():\n    title.append(row.title)\n    key_i = []\n    text_i = ''\n    for key in keywords:\n        if row[key] !=' ':\n            if key == 'hook': key= 'chorus' \n            key_i.append(key)\n            #print(index)\n            text_i +=re.sub('(\\n)+', ' <Newline> ',row[key])\n    text.append(text_i)\n    keylist.append(\",\".join(key_i))\nd = []\nfor i in range(0,len(data)):\n    d.append('<|endoftext|>' + title[i] + \\\n                '<|endoftext|>'+ keylist[i] + '<|endoftext|>'+ \\\n                text[i] + '<|endoftext|>')\n        ","metadata":{}},{"cell_type":"markdown","source":"\nlength = len(d)\ntrain_data = ' <Newsong> '.join(d[:int(length*0.6)])\ndev_data = ' <Newsong> '.join(d[int(length*0.6):int(length*0.8)])\ntest_data = ' <Newsong> '.join(d[int(length*0.8):])\n\nwith open('./train_dataset.txt','w') as f:\n    f.write(train_data)\n\nwith open('./dev_dataset.txt','w') as f:\n    f.write(dev_data)","metadata":{}},{"cell_type":"markdown","source":"train_path = './train_dataset.txt'\ndev_path = './dev_dataset.txt'","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title = \"We don't talk anymore\"\n#kw = myDataset.join_keywords(keywords, randomize=False)\n\nprompt =  SPECIAL_TOKENS['bos_token'] + title +  SPECIAL_TOKENS['sep_token'] + 'intro, verse, chorus' +SPECIAL_TOKENS['sep_token']  \ngenerated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\ndevice = torch.device(\"cuda\")\ngenerated = generated.to(device)\n\nmodel.eval();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_outputs = model.generate(generated, \n                                do_sample=True,   \n                                min_length=300, \n                                max_length=500,\n                                top_k=30,                                 \n                                top_p=0.7,        \n                                temperature=0.9,\n                                repetition_penalty=2.0,\n                                num_return_sequences=1\n                                )\n\nfor i, sample_output in enumerate(sample_outputs):\n    text = tokenizer.decode(sample_output, skip_special_tokens=False)\n    #a = len(title) + len(','.join(keywords))    \n    print(\"{}: {}\\n\\n\".format(i+1,  text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = []\nfor index, row in df.iloc[(deving_size+training_size):].iterrows():\n    prompt =  SPECIAL_TOKENS['bos_token'] + row.title +  SPECIAL_TOKENS['sep_token'] + 'intro, verse, chorus' +SPECIAL_TOKENS['sep_token']           \n    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n    device = torch.device(\"cuda\")\n    generated = generated.to(device)\n\n    sample_outputs = model.generate(generated, \n                                do_sample=True,   \n                                min_length=400, \n                                max_length=500,\n                                top_k=30,                                 \n                                top_p=0.7,        \n                                temperature=0.9,\n                                repetition_penalty=2.0,\n                                num_return_sequences=1\n                                )\n    text = tokenizer.decode(sample_outputs[0], skip_special_tokens=False)\n    output.append(text)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nnew_dict = {'output_song_with_structure':output}\nwith open(\"./output_song_with_structure.json\",\"w\") as f:\n    json.dump(new_dict,f)\n    print(\"加载入文件完成...\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}