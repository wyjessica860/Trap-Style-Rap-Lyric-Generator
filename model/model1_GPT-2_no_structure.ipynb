{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1, data processing: <br>\n    with a dataframe:","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('../input/sample/wn21630project_lyrics_input.csv')\ndata = data[['singer','title','lyrics']]\nsection_list = ['intro','verse','chorus','bridge','hook','outro','pre_chorus','post_chorus']\nlyric_aftercleaning={'title':[]}\nfor i in section_list:\n    lyric_aftercleaning[i] = []\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['lyrics'] = data['lyrics'].apply(lambda x: x.lower())\ndata['lyrics'] = data['lyrics'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\nprocessed_data = pd.DataFrame(lyric_aftercleaning)\nprocessed_data = pd.merge(processed_data,data,on='title',how='right')\nprocessed_data = processed_data.drop_duplicates('lyrics','first')\nprocessed_data.fillna('',inplace = True)\nprocessed_data.reset_index(drop=True, inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Separate the lyrics by its structure","metadata":{}},{"cell_type":"code","source":"section = ''\ntempstr = ''\nfor index, row_i in processed_data.iterrows():\n    text_start_flag = False\n    section_start_flag = False\n    doc = re.split('(\\[|\\])',row_i.lyrics)\n    for token in doc:\n        if token ==\"[\":\n            pre_or_post = ''\n            text_start_flag = False\n            section_start_flag = True\n            continue\n        if token == ']':\n            text_start_flag = True\n            section_start_flag = False\n            continue\n        if section_start_flag:\n            a = re.split(' ',token)\n            if a[0] in section_list:\n                section = a[0]\n                continue\n            if 'post' in token:\n                a1 = re.split(' ',re.sub('post-*','',token))\n                if a1[0]=='chorus': section = 'post_chorus'\n                elif a1[0] in section: section =a1[0]\n            if 'pre' in a:\n                a1 = re.split(' ',re.sub('pre-*','',token))\n                if a1[1]=='chorus': section = 'pre_chorus'\n                elif a1[1] in section: section =a1[1]        \n            else: section = '' \n            continue\n        if text_start_flag and section !='':\n            processed_data.iloc[index][section] = processed_data.iloc[index][section] + re.sub('\\n', ' <Newline> ', token)\n        \n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_data.iloc[0].lyrics","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"processed_data.to_csv('wn21630project_input_processed.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Generating Topical Poetry**","metadata":{}},{"cell_type":"code","source":"#https://github.com/tensorflow/nmt\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **GPT-2: no structure**","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Tokenizer,GPT2LMHeadModel\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2' )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"d = list(processed_data.lyrics.apply(lambda x : re.sub('\\n',' <Newline> ',x)))\nlength = len(d)\nwith open('./train_dataset.txt','w') as f:\n    f.write(' <Newsong> '.join(d[:int(length*0.6)]))\nwith open('./dev_dataset.txt','w') as f:\n    f.write(' <Newsong> '.join(d[int(length*0.6):int(length*0.8)]))\nwith open('./test_dataset.txt','w') as f:\n    f.write(' <Newsong> '.join(d[int(length*0.8):]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path = './train_dataset.txt'\ndev_path = './dev_dataset.txt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TextDataset,DataCollatorForLanguageModeling\n\ndef load_dataset(dev_path,test_path,tokenizer):\n    train_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=train_path,\n          block_size=32)\n        #   block_size=128)\n     \n    dev_dataset = TextDataset(\n          tokenizer=tokenizer,\n          file_path=dev_path,\n          block_size=32) \n        #   block_size=128)  \n    \n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, mlm=False,\n    )\n    return train_dataset,dev_dataset,data_collator\n\ntrain_dataset,dev_dataset,data_collator = load_dataset(train_path,dev_path,tokenizer)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nfrom transformers import GPT2LMHeadModel\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./output\", #The output directory\n    overwrite_output_dir=True, #overwrite the content of the output directory\n    num_train_epochs=4, # number of training epochs\n    per_device_train_batch_size=32, # batch size for training\n    per_device_eval_batch_size=64,  # batch size for evaluation\n    # per_device_train_batch_size=16, # batch size for training\n    # per_device_eval_batch_size=32,  # batch size for evaluation\n    eval_steps = 400, # Number of update steps between two evaluations.\n    save_steps=800, # after # steps model is saved \n    warmup_steps=500,# number of warmup steps for learning rate scheduler\n    prediction_loss_only=True,\n    )\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=dev_dataset,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\nfrom transformers import GPT2Tokenizer,GPT2LMHeadModel\nimport torch\ndevice ='cuda'\nmodel_new = GPT2LMHeadModel.from_pretrained('./output').to(device)\nmodel.eval()\n\ndef generation(prompt,num):\n    model_new.eval()\n\n\n\n    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n    generated = generated.to(device)\n\n    print(generated)\n\n    sample_outputs = model_new.generate(generated, do_sample=True,   top_k=50, max_length = 800,top_p=0.95, num_return_sequences=1\n                                    )\n    dict_new = {}\n    for i, sample_output in enumerate(sample_outputs):\n        \n        a = tokenizer.decode(sample_output, skip_special_tokens=True)\n        dict_new[num] = re.sub('<Newline>|[,*]','\\n',a)\n        num +=1\n        #print(\"{}: {}\\n\\n\".format(i, re.sub('\\sL+','\\n',a)))\n    return dict_new\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = {'1':'1'}\nlist(x.values())[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generation('I love',1).values()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_gen = pipeline('text-generation',model=model, tokenizer=GPT2Tokenizer.from_pretrained('gpt2'),config={'max_length':8000})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m = re.match(\"\\[intro.*\\] <Newline>.{20}?\",d[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nnew_songs = []\nnum = 0\nfor i in d[int(length*0.8):]:\n    try:\n        m = re.match(\"\\[intro.*\\] <Newline>.{20}?\",i)\n        prompt = re.sub('\\[.*\\] <Newline>','',m.group())\n        if m == '': prompt = 'I only love'\n        new = generation(prompt,num)\n        new_songs.append(list(new.values()))\n    except:\n        prompt = 'be a savage'\n        new = generation(prompt,num)\n        new_songs.append(list(new.values()))\n    num +=1\n    print(num)\nnew_dict = {'GPT2-no-structure':new_songs}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"./output_song_baseline.json\",\"w\") as f:\n    json.dump(new_dict,f)\n    print(\"加载入文件完成...\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}